---
title: "LLM Agents Erweiterungsmodul Computerlinguistik Sommersemester 2025"
collection: teaching
type: "Graduate course"
permalink: /teaching/2025-spring-teaching-8
venue: "Ludwig-Maximilians-Universität München, 13 Fakultät für Sprach- und Literaturwissenschaften, Department II, Centrum für Informations- und Sprachverarbeitung"
date: 2025-04-01
location: "Munich, German"
---

Serve as: Teacher

Teacher: Dr. Yunpu Ma, Beiduo Chen

## Course Description

This course contains the code for developing, pretraining, and finetuning a GPT-like LLM and is the official code repository for the book Build a Large Language Model (From Scratch).

In Build a Large Language Model (From Scratch), you'll learn and understand how large language models (LLMs) work from the inside out by coding them from the ground up, step by step. In this book, I'll guide you through creating your own LLM, explaining each stage with clear text, diagrams, and examples.

The method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. In addition, this book includes code for loading the weights of larger pretrained models for finetuning.

## Course Schedule:

Chapter Title	

Setup recommendations	

Ch 1: Understanding Large Language Models	

Ch 2: Working with Text Data	


Ch 3: Coding Attention Mechanisms	


Ch 4: Implementing a GPT Model from Scratch	


Ch 5: Pretraining on Unlabeled Data	


Ch 6: Finetuning for Text Classification	


Ch 7: Finetuning to Follow Instructions	


Appendix A: Introduction to PyTorch	


Appendix B: References and Further Reading	

Appendix C: Exercise Solutions	

Appendix D: Adding Bells and Whistles to the Training Loop	

Appendix E: Parameter-efficient Finetuning with LoRA	


## Final Evaluation:

* Final Project Submissions
* Project Presentation
* Technical Paper
* Reproducible Code
