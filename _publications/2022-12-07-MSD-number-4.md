---
title: "Wider & Closer: Mixture of Short-channel Distillers for Zero-shot Cross-lingual Named Entity Recognition"
collection: publications
permalink: /publication/2022-12-07-MSD-number-4
excerpt: 'This paper presents Mixture of Short-Channel Distillers (MSD), a novel multi-channel distillation framework for zero-shot cross-lingual NER that leverages intermediate-layer knowledge and preserves domain-invariant features. By combining multiple distillers and using unsupervised parallel domain adaptation, MSD enhances knowledge transfer efficiency. The approach achieves state-of-the-art results on four datasets across nine languages, demonstrating strong generalization and cross-domain compatibility.'
date: 2022-12-07
venue: 'Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing'
paperurl: 'https://aclanthology.org/2022.emnlp-main.345/'
citation: 'Jun-Yu Ma*, Beiduo Chen*, Jia-Chen Gu, Zhenhua Ling, Wu Guo, Quan Liu, Zhigang Chen, and Cong Liu. 2022. Wider & Closer: Mixture of Short-channel Distillers for Zero-shot Cross-lingual Named Entity Recognition. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5171â€“5183, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.'
---

## [Code](https://github.com/Mckysse/MSD)


## Abstract
Zero-shot cross-lingual named entity recognition (NER) aims at transferring knowledge from annotated and rich-resource data in source languages to unlabeled and lean-resource data in target languages. Existing mainstream methods based on the teacher-student distillation framework ignore the rich and complementary information lying in the intermediate layers of pre-trained language models, and domain-invariant information is easily lost during transfer. In this study, a mixture of short-channel distillers (MSD) method is proposed to fully interact the rich hierarchical information in the teacher model and to transfer knowledge to the student model sufficiently and efficiently. Concretely, a multi-channel distillation framework is designed for sufficient information transfer by aggregating multiple distillers as a mixture. Besides, an unsupervised method adopting parallel domain adaptation is proposed to shorten the channels between the teacher and student models to preserve domain-invariant features. Experiments on four datasets across nine languages demonstrate that the proposed method achieves new state-of-the-art performance on zero-shot cross-lingual NER and shows great generalization and compatibility across languages and fields.


## [Poster](https://mckysse.github.io/files/EMNLP2022_MSD_poster.pdf)
<object data="https://mckysse.github.io/files/EMNLP2022_MSD_poster.pdf" type="application/pdf" width="900px" height="900px">
    <embed src="https://mckysse.github.io/files/EMNLP2022_MSD_poster.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://mckysse.github.io/files/EMNLP2022_MSD_poster.pdf">Download PDF</a>.</p>
    </embed>
</object>


## [Arxiv](https://arxiv.org/pdf/2212.03506)
<object data="https://arxiv.org/pdf/2212.03506" type="application/pdf" width="900px" height="900px">
    <embed src="https://arxiv.org/pdf/2212.03506">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://arxiv.org/pdf/2212.03506">Download PDF</a>.</p>
    </embed>
</object>


## [Slides](https://mckysse.github.io/files/EMNLP2022_MSD_slides.pdf)
<object data="https://mckysse.github.io/files/EMNLP2022_MSD_slides.pdf" type="application/pdf" width="900px" height="900px">
    <embed src="https://mckysse.github.io/files/EMNLP2022_MSD_slides.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://mckysse.github.io/files/EMNLP2022_MSD_slides.pdf">Download PDF</a>.</p>
    </embed>
</object>

## Citation
