---
title: "Pre-training Language Model as a Multi-perspective Course Learner"
collection: publications
permalink: /publication/2023-07-09-MCL-number-5
excerpt: 'This paper proposes Multi-perspective Course Learning (MCL) to enhance ELECTRA’s pre-training by addressing its training monotony and weak generator-discriminator interaction. MCL introduces self-supervision and self-correction strategies to balance learning signals and bridge model components, along with a “course soups” method to stabilize training. The approach improves ELECTRA’s performance by up to 3.2% on GLUE and SQuAD 2.0, surpassing existing ELECTRA-style models.'
date: 2023-07-09
venue: 'Findings of the Association for Computational Linguistics: ACL 2023'
paperurl: 'https://aclanthology.org/2023.findings-acl.9/'
citation: 'Beiduo Chen, Shaohan Huang, Zihan Zhang, Wu Guo, Zhenhua Ling, Haizhen Huang, Furu Wei, Weiwei Deng, and Qi Zhang. 2023. Pre-training Language Model as a Multi-perspective Course Learner. In Findings of the Association for Computational Linguistics: ACL 2023, pages 114–128, Toronto, Canada. Association for Computational Linguistics.'
---

## [Model](https://huggingface.co/McmanusChen/MCL-base)

## Abstract
ELECTRA, the generator-discriminator pre-training framework, has achieved impressive semantic construction capability among various downstream tasks. Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning. In this study, a multi-perspective course learning (MCL) method is proposed to fetch a many degrees and visual angles for sample-efficient pre-training, and to fully leverage the relationship between generator and discriminator. Concretely, three self-supervision courses are designed to alleviate inherent flaws of MLM and balance the label in a multi-perspective way. Besides, two self-correction courses are proposed to bridge the chasm between the two encoders by creating a “correction notebook” for secondary-supervision. Moreover, a course soups trial is conducted to solve the “tug-of-war” dynamics problem of MCL, evolving a stronger pre-trained model. Experimental results show that our method significantly improves ELECTRA’s average performance by 2.8% and 3.2% absolute points respectively on GLUE and SQuAD 2.0 benchmarks, and overshadows recent advanced ELECTRA-style models under the same settings. The pre-trained MCL model is available at https://huggingface.co/McmanusChen/MCL-base.

## [Poster](https://mckysse.github.io/files/ACL2023_MCL_poster.pdf)
<object data="https://mckysse.github.io/files/ACL2023_MCL_poster.pdf" type="application/pdf" width="900px" height="900px">
    <embed src="https://mckysse.github.io/files/ACL2023_MCL_poster.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://mckysse.github.io/files/ACL2023_MCL_poster.pdf">Download PDF</a>.</p>
    </embed>
</object>


## [Arxiv](https://arxiv.org/pdf/2305.03981)
<object data="https://arxiv.org/pdf/2305.03981" type="application/pdf" width="900px" height="900px">
    <embed src="https://arxiv.org/pdf/2305.03981">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://arxiv.org/pdf/2305.03981">Download PDF</a>.</p>
    </embed>
</object>


## [Slides](https://mckysse.github.io/files/ACL2023_MCL_slides.pdf)
<object data="https://mckysse.github.io/files/ACL2023_MCL_slides.pdf" type="application/pdf" width="900px" height="900px">
    <embed src="https://mckysse.github.io/files/ACL2023_MCL_slides.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://mckysse.github.io/files/ACL2023_MCL_slides.pdf">Download PDF</a>.</p>
    </embed>
</object>

## Citation
